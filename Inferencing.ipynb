{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inferencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shining\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Audio, DatasetDict, concatenate_datasets\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import librosa\n",
    "from IPython.display import Audio\n",
    "from scipy.signal import medfilt\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "from transformers import WhisperTokenizer, WhisperFeatureExtractor, WhisperProcessor, WhisperForConditionalGeneration\n",
    "#!pip install librosa soundfile (if not installed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set CUDA to CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: ('cpu', True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "#device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\", torch.cuda.is_available()\n",
    "print(f\"Using device: {device}\") \n",
    "\n",
    "import GPUtil\n",
    "GPUtil.getAvailable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 450 examples [00:00, 43765.64 examples/s]\n",
      "Generating test split: 57 examples [00:00, 18996.85 examples/s]\n",
      "Generating validation split: 56 examples [00:00, 27982.01 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available splits: dict_keys(['train', 'test', 'validation'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Dataset\n",
    "minds_us_data = load_dataset('csv', data_files={\n",
    "    'train': 'F:\\\\AI Portfolio Project\\\\Project-3\\\\datasets_split\\\\minds_traindf.csv',\n",
    "    'test': 'F:\\\\AI Portfolio Project\\\\Project-3\\\\datasets_split\\\\minds_testdf.csv',\n",
    "    'validation': 'F:\\\\AI Portfolio Project\\\\Project-3\\\\datasets_split\\\\minds_valdf.csv'\n",
    "})\n",
    "\n",
    "# Print available keys to check what splits are loaded\n",
    "print(\"Available splits:\", minds_us_data.keys())\n",
    "\n",
    "# Combine the datasets into one DatasetDict\n",
    "ds = DatasetDict({\n",
    "    'train': minds_us_data['train'],\n",
    "    'test': minds_us_data.get('test'),  # Use .get() to avoid KeyError if 'test' doesn't exist\n",
    "    'valid': minds_us_data.get('validation')\n",
    "})\n",
    "\n",
    "# Check if 'test' split exists before proceeding\n",
    "if ds['test'] is None:\n",
    "    print(\"Warning: 'test' split not found. Please check your dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filepath', 'text_asr', 'text_translated', 'intent'],\n",
      "        num_rows: 450\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filepath', 'text_asr', 'text_translated', 'intent'],\n",
      "        num_rows: 57\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['filepath', 'text_asr', 'text_translated', 'intent'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Load Audio\n",
    "def load_audio_data(batch, audio_base_path):\n",
    "    audio_files = [os.path.join(audio_base_path, filepath) for filepath in batch['filepath']]\n",
    "    audio_data = [librosa.load(file_path, sr=None) for file_path in audio_files]\n",
    "    \n",
    "    # Separate audio data and sample rates\n",
    "    audio_signals = [data[0] for data in audio_data]\n",
    "    sample_rates = [data[1] for data in audio_data]\n",
    "    \n",
    "    batch['audio'] = [{'path': file_path, 'array': audio, 'sampling_rate': sr} for file_path, audio, sr in zip(audio_files, audio_signals, sample_rates)]\n",
    "    return batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 450/450 [00:04<00:00, 103.40 examples/s]\n",
      "Map: 100%|██████████| 57/57 [00:00<00:00, 153.46 examples/s]\n",
      "Map: 100%|██████████| 56/56 [00:00<00:00, 142.59 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filepath', 'text_asr', 'text_translated', 'intent', 'audio'],\n",
      "        num_rows: 450\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filepath', 'text_asr', 'text_translated', 'intent', 'audio'],\n",
      "        num_rows: 57\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['filepath', 'text_asr', 'text_translated', 'intent', 'audio'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load Audio Path\n",
    "audio_base_path = \"F:\\\\AI Portfolio Project\\\\Project-3\\\\datasets\\\\MInDS-14\\\\audio\"\n",
    "ds = ds.map(load_audio_data, fn_kwargs={'audio_base_path': audio_base_path}, batched=True)\n",
    "\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 450/450 [00:15<00:00, 28.17 examples/s]\n",
      "Map: 100%|██████████| 57/57 [00:01<00:00, 29.08 examples/s]\n",
      "Map: 100%|██████████| 56/56 [00:01<00:00, 32.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def update_audio_paths(batch):\n",
    "    # Generate full path for each audio file\n",
    "    batch['audio'] = [\n",
    "        {'path': os.path.join(audio_base_path, filepath),\n",
    "         'array': audio['array'],\n",
    "         'sampling_rate': audio['sampling_rate']}\n",
    "        for filepath, audio in zip(batch['filepath'], batch['audio'])\n",
    "    ]\n",
    "    return batch\n",
    "\n",
    "# Apply the preprocessing function to the dataset\n",
    "ds = ds.map(update_audio_paths, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.remove_columns([\"text_translated\", \"intent\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['filepath', 'text_asr', 'audio'],\n",
      "        num_rows: 450\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['filepath', 'text_asr', 'audio'],\n",
      "        num_rows: 57\n",
      "    })\n",
      "    valid: Dataset({\n",
      "        features: ['filepath', 'text_asr', 'audio'],\n",
      "        num_rows: 56\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "minds_fix = ds\n",
    "print(minds_fix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting MER (Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "metric = evaluate.load(\"wer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "\n",
    "    # replace -100 with the pad_token_id\n",
    "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
    "\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
    "\n",
    "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"wer\": wer}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and processor loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "#inferencing\n",
    "import time\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor\n",
    "\n",
    "# Define the path to the model checkpoint (For local, using checkpoint-48)\n",
    "model_path = \"F:\\AI Portfolio Project\\Project-3\\output\"\n",
    "\n",
    "try:\n",
    "    model = WhisperForConditionalGeneration.from_pretrained(model_path)\n",
    "    processor = WhisperProcessor.from_pretrained(model_path)\n",
    "    print(\"Model and processor loaded successfully.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error loading model or processor: {e}\")\n",
    "\n",
    "def transcribe(audio):\n",
    "    input_features = processor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\").input_features.to(\"cpu\")\n",
    "    \n",
    "    forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"de\", task=\"transcribe\")\n",
    "    \n",
    "    # Generate predictions and ensure the model's computations are on the correct device\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features, forced_decoder_ids=forced_decoder_ids)\n",
    "    \n",
    "    # Decode the predictions into text\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)\n",
    "    return transcription[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "minds_fix = minds_fix.cast_column(\"audio\", Audio(sampling_rate=16_000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample 1:\n",
      "Reference: hi I can't use my car because it doesn't work I don't know why my payment was declined and you help\n",
      "Prediction: hi I can't use my car because it doesn't work I don't know why my payment was declined and you help\n",
      "Inference time: 2.8306 seconds\n",
      "\n",
      "Sample 2:\n",
      "Reference: hi I am trying to make a big payment online and it says I'll get a text message from you guys to confirm my identity so I guess I'm standing by whenever you send it I'll be\n",
      "Prediction: hi I am trying to make a big payment online and it says I'll get a text message from you guys to confirm my identity so I guess I'm standing by whenever you send it I'll be here and I just tell you guys the number right all right thanks\n",
      "Inference time: 3.9469 seconds\n",
      "\n",
      "Sample 3:\n",
      "Reference: transfer money to the account\n",
      "Prediction: transfer money to the account\n",
      "Inference time: 2.0175 seconds\n",
      "\n",
      "Sample 4:\n",
      "Reference: good morning I am going to be taking a trip to Germany shortly and wanted to know whether my bank card will work with\n",
      "Prediction: good morning I am going to be taking a trip to Germany shortly and want to know whether my bank card will work while I am overseas\n",
      "Inference time: 2.8596 seconds\n",
      "\n",
      "Sample 5:\n",
      "Reference: hi I've lost my bank card and please stop all transactions I don't want anybody using my card for anything I don't know where it is can you do this for me thank you\n",
      "Prediction: hi I've lost my bank card and please stop all transactions I don't want anybody using my card for anything I don't know where it is can you do this for me thank you\n",
      "Inference time: 3.2957 seconds\n",
      "\n",
      "WER for 5 samples: 0.1487603305785124\n"
     ]
    }
   ],
   "source": [
    "# Run inference on five samples\n",
    "for i in range(5):\n",
    "    sample = minds_fix[\"test\"][i]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    transcription = transcribe(sample[\"audio\"])\n",
    "    end_time = time.time()\n",
    "    \n",
    "    inference_time = end_time - start_time\n",
    "    \n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"Reference: {sample['text_asr']}\")\n",
    "    print(f\"Prediction: {transcription}\")\n",
    "    print(f\"Inference time: {inference_time:.4f} seconds\")\n",
    "    print()\n",
    "\n",
    "# Calculate overall WER for these five samples\n",
    "wer = metric.compute(predictions=[transcribe(minds_fix[\"test\"][i][\"audio\"]) for i in range(5)],\n",
    "                     references=[minds_fix[\"test\"][i][\"text_asr\"] for i in range(5)])\n",
    "print(f\"WER for 5 samples: {wer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
